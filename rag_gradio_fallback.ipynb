{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e073eb59-5957-4eb1-b437-fec2e60b2468",
   "metadata": {},
   "source": [
    "## Smart Waterways: AIS RAG Chatbot with Fallback\n",
    "\n",
    "This notebook implements a **Retrieval-Augmented Generation (RAG) chatbot** for querying **Automatic Identification System (AIS) vessel tracking data**. It integrates structured vessel records from a CSV file with a large language model (LLM), enabling natural language queries about vessel positions, attributes, and movements.\n",
    "\n",
    "### Key Features\n",
    "- **Data Ingestion & Embeddings**  \n",
    "  Loads AIS vessel records from a CSV file and embeds them using `BAAI/bge-small-en`. Data is stored and retrieved from a persistent **Chroma vector database**.\n",
    "\n",
    "- **Retrieval-Augmented Chat**  \n",
    "  Uses LangChainâ€™s `SelfQueryRetriever` and `ConversationalRetrievalChain` to interpret user queries, retrieve the most relevant vessel records, and generate context-aware answers.\n",
    "\n",
    "- **Fallback Knowledge**  \n",
    "  If no relevant AIS records are found or the model produces an uncertain answer, a **fallback chain** provides responses using general maritime knowledge.\n",
    "\n",
    "- **Conversation Memory**  \n",
    "  Maintains chat history using `ConversationBufferMemory`, allowing for multi-turn conversations with context.\n",
    "\n",
    "- **Interactive Gradio UI**  \n",
    "  A user-friendly interface built with **Gradio**, featuring:\n",
    "  - A chat window for natural queries  \n",
    "  - Support for both **Enter key** and **Submit button**  \n",
    "  - Clear button to reset the conversation  \n",
    "\n",
    "### Usage\n",
    "Run the notebook to launch a **shareable Gradio app**, where you can type vessel-related queries (e.g., positions, speed, cargo type) and get answers sourced either from the AIS dataset or from general maritime expertise when data is missing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "007f1411-209f-4ff6-8323-2824406bfd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://37e532ae76d2f79c8e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://37e532ae76d2f79c8e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------------------\n",
    "# Setup: Embeddings + CSV Loader\n",
    "# ----------------------------\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# meta_columns = ['MMSI','VesselName', 'CallSign']\n",
    "meta_columns = ['MMSI', 'BaseDateTime', 'LAT', 'LON', 'SOG', 'COG', 'Heading',\n",
    "       'VesselName', 'IMO', 'CallSign', 'VesselType', 'Status', 'Length',\n",
    "       'Width', 'Draft', 'Cargo', 'TransceiverClass']\n",
    "all_columns = ['MMSI', 'BaseDateTime', 'LAT', 'LON', 'SOG', 'COG', 'Heading',\n",
    "       'VesselName', 'IMO', 'CallSign', 'VesselType', 'Status', 'Length',\n",
    "       'Width', 'Draft', 'Cargo', 'TransceiverClass']\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=\"AIS_sampleData2.csv\",\n",
    "    metadata_columns=meta_columns,\n",
    "    content_columns=all_columns,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "doc_db = Chroma(\n",
    "    persist_directory=\"AIS_sampleData2_db_V2\",\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "\n",
    "#run the following line for the first time when embeddings are being created.\n",
    "# doc_db.add_documents(documents)\n",
    "\n",
    "# ----------------------------\n",
    "# Setup: LLM + Retriever\n",
    "# ----------------------------\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(name=\"MMSI\", description=\"Unique Maritime Mobile Service Identity number of the vessel\", type=\"string\"),\n",
    "    AttributeInfo(name=\"BaseDateTime\", description=\"UTC date and time of the report and the vessel record\", type=\"string\"),\n",
    "    AttributeInfo(name=\"LAT\", description=\"Latitude of the vessel's position\", type=\"float\"),\n",
    "    AttributeInfo(name=\"LON\", description=\"Longitude of the vessel's position\", type=\"float\"),\n",
    "    AttributeInfo(name=\"SOG\", description=\"Speed Over Ground in knots\", type=\"float\"),\n",
    "    AttributeInfo(name=\"COG\", description=\"Course Over Ground in degrees\", type=\"float\"),\n",
    "    AttributeInfo(name=\"Heading\", description=\"Vessel's true heading in degrees (0-359 or 511 for not available)\", type=\"integer\"),\n",
    "    AttributeInfo(name=\"VesselName\", description=\"Name of the vessel\", type=\"string\"),\n",
    "    AttributeInfo(name=\"IMO\", description=\"International Maritime Organization number\", type=\"string\"),\n",
    "    AttributeInfo(name=\"CallSign\", description=\"Vessel's radio call sign\", type=\"string\"),\n",
    "    AttributeInfo(name=\"VesselType\", description=\"Type of vessel (e.g., Cargo, Tanker, Passenger, Fishing)\", type=\"string\"),\n",
    "    AttributeInfo(name=\"Status\", description=\"Navigational status of the vessel (e.g., Underway, Anchored, Moored)\", type=\"string\"),\n",
    "    AttributeInfo(name=\"Length\", description=\"Length of the vessel in meters\", type=\"integer\"),\n",
    "    AttributeInfo(name=\"Width\", description=\"Width of the vessel in meters\", type=\"integer\"),\n",
    "    AttributeInfo(name=\"Draft\", description=\"Current static draft of the vessel in meters\", type=\"float\"),\n",
    "    AttributeInfo(name=\"Cargo\", description=\"Type of cargo carried by the vessel\", type=\"string\"),\n",
    "    AttributeInfo(name=\"TransceiverClass\", description=\"AIS transceiver class (A or B)\", type=\"string\"),\n",
    "    # Add other metadata fields if 'source' or 'row' are important for querying\n",
    "    AttributeInfo(name=\"source\", description=\"The source CSV file name\", type=\"string\"),\n",
    "    AttributeInfo(name=\"row\", description=\"Row number in the original CSV file\", type=\"integer\"),\n",
    "]\n",
    "\n",
    "document_content_description = \"AIS data for vessel records\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    doc_db,\n",
    "    document_content_description,\n",
    "    metadata_field_info\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Setup: Fallback Chain\n",
    "# ----------------------------\n",
    "\n",
    "fallback_prompt = PromptTemplate.from_template(\n",
    "    \"You are a maritime expert. Answer the question based on your general maritime knowledge.\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "\n",
    "fallback_chain = LLMChain(llm=llm, prompt=fallback_prompt)\n",
    "\n",
    "# ----------------------------\n",
    "# Chat Handler with Fallback\n",
    "# ----------------------------\n",
    "\n",
    "def chat_interface(user_input, history):\n",
    "    response = conversational_chain.invoke({\"question\": user_input})\n",
    "    answer = response[\"answer\"].strip()\n",
    "\n",
    "    # no_docs = len(response.get(\"source_documents\", [])) == 0\n",
    "    fresh_docs = retriever.get_relevant_documents(user_input)\n",
    "    no_docs = len(fresh_docs) == 0\n",
    "    trigger_phrases = [\n",
    "        \"i don't know\",\n",
    "        \"i am not sure\",\n",
    "        \"no relevant information\",\n",
    "        \"not provided in the context\",\n",
    "        \"context does not provide\",\n",
    "        \"no information found\"\n",
    "    ]\n",
    "    poor_response = any(phrase in answer.lower() for phrase in trigger_phrases)\n",
    "    \n",
    "    if no_docs or poor_response:\n",
    "        answer = fallback_chain.run({\"question\": user_input})\n",
    "        answer += \"\\n\\n_(Answered using general maritime knowledge)_\"\n",
    "    else:\n",
    "        answer += \"\\n\\n_(Answered using AIS database)_\"\n",
    "\n",
    "    history.append((user_input, answer))\n",
    "    return history, history\n",
    "\n",
    "# ----------------------------\n",
    "# Gradio UI\n",
    "# ----------------------------\n",
    "\n",
    "with gr.Blocks(title=\"AIS RAG Chatbot\") as demo:\n",
    "    # Top title centered\n",
    "    gr.Markdown(\n",
    "        \"<h1 style='text-align: center;'>Smart Waterways: A Retrieval-Augmented AI Framework for Vessel Tracking and Decision Support</h1>\"\n",
    "    )\n",
    "\n",
    "    # gr.Markdown(\"### ðŸš¢ AIS Vessel Query Assistant\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"AIS Vessel Query Assistant\")\n",
    "    message = gr.Textbox(placeholder=\"Type your question here...\", show_label=False)\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    def respond(user_input, chat_history):\n",
    "        return chat_interface(user_input, chat_history)\n",
    "\n",
    "    # Handle submit via Enter key\n",
    "    message.submit(respond, [message, state], [chatbot, state])\n",
    "    # Handle submit via button click\n",
    "    submit_btn.click(respond, [message, state], [chatbot, state])\n",
    "    \n",
    "    #Clear textbox after submit (both Enter and Button)\n",
    "    message.submit(lambda *args: \"\", None, message)\n",
    "    submit_btn.click(lambda *args: \"\", None, message)\n",
    "\n",
    "    gr.ClearButton([message, chatbot, state])\n",
    "\n",
    "    # Footer info\n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style='text-align: center; font-size:18px;'>\n",
    "        <hr>\n",
    "        <strong>Smart Rivers 2025</strong><br><br>\n",
    "        Presented by<br>\n",
    "        <strong>Zaidur Rahman</strong> & <strong>Dr. Heather Nachtmann</strong><br>\n",
    "        <img src=\"https://brand.uark.edu/_resources/images/UA_Logo.png\" \n",
    "             alt=\"University of Arkansas Logo\" \n",
    "             width=\"120\" \n",
    "             style=\"display:block; margin-left:auto; margin-right:auto;\">\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73488d2b-31aa-40b7-abfb-204f6920050e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_rag)",
   "language": "python",
   "name": "new_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
